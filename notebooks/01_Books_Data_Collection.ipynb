{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeebce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../var.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pdfplumber\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c700e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_single_entry(text, category, date):\n",
    "    \"\"\"\n",
    "    Get the details for each book from the text passed.\n",
    "    Matching the appropriate details and storing them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rank_match = re.match(r\"(\\d+)\", text)\n",
    "        rank = int(rank_match.group(1)) if rank_match else None\n",
    "\n",
    "        title_match = re.search(r\"\\d+\\s+(.*?), by\", text)\n",
    "        title = title_match.group(1).title() if title_match else None\n",
    "\n",
    "        author_match = re.search(r\"by (.*?)\\.\", text)\n",
    "        author = author_match.group(1).strip() if author_match else None\n",
    "\n",
    "        pub_match = re.search(r\"\\((.*?)\\)\", text)\n",
    "        publisher = pub_match.group(1).strip() if pub_match else None\n",
    "        \n",
    "        rank_pairs = re.findall(r\"(?:(\\d{1,2})|--)\\s+(\\d{1,2})\", text)\n",
    "        if rank_pairs:\n",
    "            first = rank_pairs[0]\n",
    "            last_week_rank = int(first[0]) if first[0] and first[0].isdigit() else None\n",
    "            weeks_on_list = int(first[1])\n",
    "        else:\n",
    "            last_week_rank = None\n",
    "            weeks_on_list = None\n",
    "\n",
    "        text = re.sub(r\"(?:(\\d{1,2})|--)\\s+(\\d{1,2})\", \"\", text, count=1).strip()\n",
    "\n",
    "        return {\n",
    "            \"date\": date,\n",
    "            \"category\": category,\n",
    "            \"rank\": rank,\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"publisher\": publisher,\n",
    "            \"last_week_rank\": last_week_rank,\n",
    "            \"weeks_on_list\": weeks_on_list\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse entry: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bestseller_pdf(filepath):\n",
    "    \n",
    "    \"\"\"\n",
    "    Store the list for each file.\n",
    "    Matching each entry to it's appropriate format and storing them correctly.\n",
    "    \"\"\"\n",
    "    buffer = []\n",
    "    entries = []\n",
    "\n",
    "    date_match = re.search(r\"s_(.*?)\\.\", filepath)\n",
    "    date = date_match.group(1) if date_match else None\n",
    "    category = None\n",
    "    \n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            \n",
    "            try:\n",
    "                text = page.extract_text()\n",
    "                lines = text.split(\"\\n\")[2:]\n",
    "\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    category =  re.match(r\"Week (.*?) Week On List$\", line, re.IGNORECASE)\n",
    "                    if category:\n",
    "                        category = category.group(1).strip().title()\n",
    "                        continue\n",
    "\n",
    "                    elif re.match(r\"^\\d{1,2}\\s\", line):\n",
    "                        if buffer:\n",
    "                            full_entry = \" \".join(buffer)\n",
    "                            parsed = parse_single_entry(full_entry, category, date)\n",
    "                            if parsed:\n",
    "                                entries.append(parsed)\n",
    "                            buffer = []\n",
    "\n",
    "                    buffer.append(line)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping bad page in {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if buffer:\n",
    "            full_entry = \" \".join(buffer)\n",
    "            parsed = parse_single_entry(full_entry, category, date)\n",
    "            if parsed:\n",
    "                entries.append(parsed)\n",
    "        \n",
    "    cleaned_entries = [e for e in entries if e is not None]\n",
    "    return cleaned_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707fc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(filepath):\n",
    "    \"\"\"\n",
    "    Helper function to accelerate the parsing of all data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parse_bestseller_pdf(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "pdf_folder = \"../pdfs\"\n",
    "filepaths = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "all_entries = []\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for result in tqdm(executor.map(parse_pdf, filepaths), total=len(filepaths)):\n",
    "            if result:\n",
    "                all_entries.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = pd.DataFrame([e for e in all_entries if e is not None])\n",
    "books_data.to_csv(\"../data/raw/nyt_bestsellers_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ce750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d813ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e90918",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data[\"date\"] = pd.to_datetime(books_data[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data[\"season\"] = books_data[\"date\"].apply(get_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c14ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data.to_csv(\"../data/raw/nyt_bestsellers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_detail_generator(batch):\n",
    "    \"\"\"\n",
    "    Query Google Books API for public information \n",
    "    about each book in the intended database.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "    \n",
    "    for entry in tqdm(batch):\n",
    "        query = f'intitle:{entry[\"title\"]}'\n",
    "        if entry[\"author\"]:\n",
    "            query += f'+inauthor:{entry[\"author\"]}'\n",
    "            \n",
    "\n",
    "        books_url = f\"{base_url}?q={query}&maxResults=1&key={books_key}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(books_url, timeout=10)       \n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            print(f\"Failed to get book info{error}\")\n",
    "        if response and response.ok: \n",
    "            try:\n",
    "                data = response.json()\n",
    "                if \"items\" in data:\n",
    "                    result = data[\"items\"][0][\"volumeInfo\"]\n",
    "                    entry[\"maturityRating\"] = result.get(\"maturityRating\") if \"maturityRating\" in result else None\n",
    "                    entry[\"description\"] = result.get(\"description\") if \"description\" in result else None\n",
    "                    entry[\"categories\"] = result.get(\"categories\") if \"categories\" in result else None\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        time.sleep(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd753f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from more_itertools import chunked\n",
    "books_key = os.getenv(\"BOOKS_KEY\")\n",
    "\n",
    "first_batch = False\n",
    "batches = list(chunked(all_entries, 1000))\n",
    "for batch in batches:\n",
    "    book_detail_generator(batch)\n",
    "    batch_data = pd.DataFrame([e for e in batch if e is not None])\n",
    "    if first_batch:\n",
    "        batch_data.to_csv(\"../data/raw/nyt_bestsllers_detailed.csv\",mode=\"w\",index=False)\n",
    "        first_batch = False\n",
    "    else:\n",
    "        batch_data.to_csv(\"../data/raw/nyt_bestsellers_detailed.csv\", mode=\"a\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
